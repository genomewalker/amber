# AMBER Paper Workflow — Unified Pipeline
# Parts: A-binning → B-QC → C-deconvolve → D-phylogenomics → E-BEAST2

import os
from pathlib import Path

configfile: "config.yaml"

# ---------------------------------------------------------------------------
# Paths and constants
# ---------------------------------------------------------------------------
_TS = config["timestamp"]
OUTDIR = f"{config['outdir']}/{_TS}" if _TS != "default" else config["outdir"]

SKIP_BINNING = config.get("skip_binning", False)
RESOLVE_DIR  = config["existing_resolve_dir"] if SKIP_BINNING else f"{OUTDIR}/resolve"
# Existing resolve dirs have bins directly in root; new runs put them in bins/
RESOLVE_BINS = RESOLVE_DIR if SKIP_BINNING else f"{OUTDIR}/resolve/bins"

PHYLO_DIR = f"{OUTDIR}/phylogenomics"

ENV_ASSEMBLY = config["envs"]["assembly"]
ENV_GUNC     = config["envs"]["gunc"]
ENV_GTDBTK   = config["envs"]["gtdbtk"]

MARKERS      = config["markers"]
SPECIES_TAXA = config["species_level_taxa"] + config["outgroup_taxa"]

# Reference genome names — discovered upfront from the reference directory
_ref_dir = Path(config["reference_genomes_dir"])
REF_GENOMES = sorted(set(
    p.stem
    for ext in [".fna", ".fa"]
    for p in _ref_dir.glob(f"*{ext}")
))
PHYLO_SAMPLES = REF_GENOMES + ["ancient_consensus", "modern_consensus"]
if config.get("incubation", {}).get("include_s17_assembly", False):
    PHYLO_SAMPLES += ["s17_ancient_relative"]

N_REPS = config["amber"].get("n_reps", 3)
REPS   = list(range(1, N_REPS + 1))


# ---------------------------------------------------------------------------
# Target rules
# ---------------------------------------------------------------------------

localrules: link_ref_genome, link_ancient_consensus, link_modern_consensus, link_s17_assembly,
    checkm2_rep, checkm2_final, fastani_bins, fastani_deconvolved,
    gunc, gtdbtk, summary, deconvolve, amber_resolve,
    prodigal, hmmsearch, extract_markers, align_marker, trim_alignment,
    back_translate, concatenate_aa, trim_concat_aa, concatenate_codon, trim_concat_codon, codon_12only,
    beast2_xml, beast2_snp_xml,
    nucmer_align, extract_snps, build_snp_matrix,
    treeannotator, treeannotator_snp,
    validate_stasis,
    combine_incubation_bins

rule all:
    """Full pipeline: binning → QC → deconvolve → phylogenomics → BEAST2 XML"""
    input:
        f"{OUTDIR}/summary.tsv",
        f"{OUTDIR}/fastani/bins_vs_refs.tsv",
        f"{OUTDIR}/deconvolve/ancient_consensus.fa",
        f"{OUTDIR}/fastani/deconvolved_vs_refs.tsv",
        f"{PHYLO_DIR}/08_trees/codon_ml_tree.treefile",
        f"{PHYLO_DIR}/09_beast2/codon_tipdating.xml",
        f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_tree.treefile"

rule fastani_only:
    """FastANI bins vs refs + deconvolved vs refs"""
    input:
        f"{OUTDIR}/fastani/bins_vs_refs.tsv",
        f"{OUTDIR}/fastani/deconvolved_vs_refs.tsv"

rule bin_resolve:
    """Run 3 independent amber_bin reps and amber_resolve consensus"""
    input:
        bins   = f"{OUTDIR}/resolve/bins",
        damage = f"{OUTDIR}/resolve/damage_per_bin.tsv",
        stats  = f"{OUTDIR}/resolve/bin_stats.tsv"

rule qc_only:
    """Run CheckM2 + GUNC on the resolve bins (validate HQ bins)"""
    input:
        f"{OUTDIR}/checkm2_final/quality_report.tsv",
        f"{OUTDIR}/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv"

rule qc_full:
    """Full QC including GTDB-Tk + summary table"""
    input:
        f"{OUTDIR}/summary.tsv"

rule phylo:
    """Phylogenomics + BEAST2 XML (no BEAST2 run)"""
    input:
        f"{PHYLO_DIR}/08_trees/aa_ml_tree.treefile",
        f"{PHYLO_DIR}/08_trees/codon_ml_tree.treefile",
        f"{PHYLO_DIR}/08_trees/codon12_ml_tree.treefile",
        f"{PHYLO_DIR}/09_beast2/codon_tipdating.xml",
        f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_tree.treefile"

rule scf:
    """Site concordance factors for codon and SNP trees (robustness check)"""
    input:
        f"{PHYLO_DIR}/08_trees/codon_scf.cf.tree",
        f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_scf.cf.tree"

rule beast2_snp:
    """SNP-based BEAST2 tip-dating with ASC correction (+ stasis validation)"""
    input:
        f"{PHYLO_DIR}/09_beast2/snp_tipdating_mcc.tree",
        f"{PHYLO_DIR}/validation/stasis_validation_report.txt"

rule stasis_validate:
    """Extreme validation of stasis claim (no BEAST2 required)"""
    input:
        f"{PHYLO_DIR}/validation/stasis_validation_report.txt"


# ---------------------------------------------------------------------------
# PART A: Binning — skipped when skip_binning: true
# ---------------------------------------------------------------------------

if not SKIP_BINNING:

    rule amber_bin:
        input:
            contigs = config["contigs"],
            bam     = config["bam"],
            hmm     = config["amber"]["hmm"]
        output:
            bins = directory(f"{OUTDIR}/rep{{rep}}/bins")
        params:
            amber              = config["amber"]["binary"],
            encoder_seed       = config["amber"]["encoder_seed"],
            random_seed        = config["amber"]["random_seed"],
            resolution         = config["amber"]["resolution"],
            bandwidth          = config["amber"]["bandwidth"],
            partgraph_ratio    = config["amber"]["partgraph_ratio"],
            n_leiden_restarts  = config["amber"]["n_leiden_restarts"],
            n_encoder_restarts = config["amber"]["n_encoder_restarts"],
            env                = ENV_ASSEMBLY
        threads: config["threads"]
        resources:
            mem_mb     = 64000,
            runtime    = 180,
            slurm_partition = "compregular",
            gres        = "gpu:a100:1"
        log: f"{OUTDIR}/logs/amber_rep{{rep}}.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            {params.amber} bin \
                --contigs {input.contigs} \
                --bam {input.bam} \
                --hmm {input.hmm} \
                --encoder-seed {params.encoder_seed} \
                --random-seed {params.random_seed} \
                --resolution {params.resolution} \
                --bandwidth {params.bandwidth} \
                --partgraph-ratio {params.partgraph_ratio} \
                --leiden-restarts {params.n_leiden_restarts} \
                --encoder-restarts {params.n_encoder_restarts} \
                --threads {threads} \
                --output {output.bins} \
                2>&1 | tee {log}
            """

    rule checkm2_rep:
        input:
            bins = f"{OUTDIR}/rep{{rep}}/bins"
        output:
            report = f"{OUTDIR}/rep{{rep}}/checkm2/quality_report.tsv"
        params:
            outdir = f"{OUTDIR}/rep{{rep}}/checkm2",
            env    = ENV_ASSEMBLY
        threads: config["checkm2"]["threads"]
        resources:
            mem_mb   = 32000,
            runtime  = 60,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/checkm2_rep{{rep}}.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            checkm2 predict \
                --input {input.bins} \
                --output-directory {params.outdir} \
                --extension fa \
                --threads {threads} \
                --force \
                2>&1 | tee {log}
            """

    rule amber_resolve:
        input:
            bins_dirs = expand(f"{OUTDIR}/rep{{rep}}/bins", rep=REPS),
            contigs   = config["contigs"]
        output:
            bins   = directory(f"{OUTDIR}/resolve/bins"),
            damage = f"{OUTDIR}/resolve/damage_per_bin.tsv",
            smiley = f"{OUTDIR}/resolve/smiley_plot_rates.tsv",
            stats  = f"{OUTDIR}/resolve/bin_stats.tsv"
        params:
            amber          = config["amber"]["binary"],
            abins          = lambda wc, input: " ".join(f"{d}/run.abin" for d in input.bins_dirs),
            min_cobin_frac = config["resolve"]["min_cobin_frac"],
            restarts       = config["resolve"]["restarts"],
            env            = ENV_ASSEMBLY
        threads: config["threads"]
        resources:
            mem_mb   = 32000,
            runtime  = 30,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/resolve.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH
            export OMP_NUM_THREADS={threads}

            {params.amber} resolve \
                --runs {params.abins} \
                --contigs {input.contigs} \
                --min-cobin-frac {params.min_cobin_frac} \
                --restarts {params.restarts} \
                --output {output.bins} \
                2>&1 | tee {log}

            mv {output.bins}/damage_per_bin.tsv {output.damage}
            mv {output.bins}/smiley_plot_rates.tsv {output.smiley}
            mv {output.bins}/bin_stats.tsv {output.stats}
            """


# ---------------------------------------------------------------------------
# PART B: Quality Control
# ---------------------------------------------------------------------------

rule checkm2_final:
    input:
        bins = f"{RESOLVE_BINS}"
    output:
        report = f"{OUTDIR}/checkm2_final/quality_report.tsv"
    params:
        outdir = f"{OUTDIR}/checkm2_final",
        env    = ENV_ASSEMBLY
    threads: config["checkm2"]["threads"]
    resources:
        mem_mb   = 32000,
        runtime  = 60,
        slurm_partition = "compregular"
    log: f"{OUTDIR}/logs/checkm2_final.log"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

        checkm2 predict \
            --input {input.bins} \
            --output-directory {params.outdir} \
            --extension fa \
            --threads {threads} \
            --force \
            2>&1 | tee {log}
        """


checkpoint filter_hq_mq:
    """Symlink bins passing completeness/contamination threshold into hq_mq_bins/"""
    input:
        bins  = f"{RESOLVE_BINS}",
        checkm = f"{OUTDIR}/checkm2_final/quality_report.tsv"
    output:
        hq_mq = directory(f"{OUTDIR}/hq_mq_bins")
    params:
        min_completeness = config["filter"]["min_completeness"],
        max_contamination = config["filter"]["max_contamination"]
    run:
        import os, csv
        os.makedirs(output.hq_mq, exist_ok=True)
        n = 0
        with open(input.checkm) as f:
            for row in csv.DictReader(f, delimiter='\t'):
                if (float(row['Completeness']) >= params.min_completeness and
                        float(row['Contamination']) <= params.max_contamination):
                    src = os.path.abspath(os.path.join(input.bins, row['Name'] + '.fa'))
                    dst = os.path.join(output.hq_mq, row['Name'] + '.fa')
                    if os.path.exists(src) and not os.path.exists(dst):
                        os.symlink(src, dst)
                        n += 1
        print(f"[filter_hq_mq] {n} bins passed "
              f"(comp>={params.min_completeness}%, cont<={params.max_contamination}%)")


def get_hq_mq_bins(wildcards):
    checkpoints.filter_hq_mq.get(**wildcards)
    return f"{OUTDIR}/hq_mq_bins"


rule fastani_bins:
    """FastANI: compare all HQ/MQ bins against reference Methanoflorentales genomes"""
    input:
        bins = get_hq_mq_bins
    output:
        ani = f"{OUTDIR}/fastani/bins_vs_refs.tsv"
    params:
        ref_dir = config["reference_genomes_dir"]
    conda: "/maps/projects/fernandezguerra/apps/opt/conda/envs/assembly"
    threads: config["threads"]
    resources:
        mem_mb   = 16000,
        runtime  = 60,
        slurm_partition = "compregular"
    log: f"{OUTDIR}/logs/fastani_bins.log"
    shell:
        """
        mkdir -p $(dirname {output.ani})
        find {input.bins} -name "*.fa" -o -name "*.fna" > {output.ani}.ql
        find {params.ref_dir} -name "*.fna" -o -name "*.fa" > {output.ani}.rl
        fastANI --ql {output.ani}.ql --rl {output.ani}.rl \
            -o {output.ani} -t {threads} 2> {log}
        rm -f {output.ani}.ql {output.ani}.rl
        """


rule gunc:
    """GUNC chimera detection — runs only on HQ/MQ bins"""
    input:
        bins = get_hq_mq_bins
    output:
        report = f"{OUTDIR}/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv"
    params:
        outdir = f"{OUTDIR}/gunc",
        db     = config["gunc"]["db"],
        env    = ENV_GUNC
    threads: config["gunc"]["threads"]
    resources:
        mem_mb   = 32000,
        runtime  = 240,
        slurm_partition = "compregular"
    log: f"{OUTDIR}/logs/gunc.log"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

        mkdir -p {params.outdir}
        gunc run \
            --input_dir {input.bins} \
            --out_dir {params.outdir} \
            --db_file {params.db} \
            --threads {threads} \
            2>&1 | tee {log}
        """


rule gtdbtk:
    """GTDB-Tk taxonomic classification — runs only on HQ/MQ bins"""
    input:
        bins = get_hq_mq_bins
    output:
        ar53   = f"{OUTDIR}/gtdbtk/gtdbtk.ar53.summary.tsv",
        bac120 = f"{OUTDIR}/gtdbtk/gtdbtk.bac120.summary.tsv"
    params:
        outdir = f"{OUTDIR}/gtdbtk",
        db     = config["gtdbtk"]["db"],
        env    = ENV_GTDBTK
    threads: config["gtdbtk"]["threads"]
    resources:
        mem_mb   = 128000,
        runtime  = 240,
        slurm_partition = "compregular"
    log: f"{OUTDIR}/logs/gtdbtk.log"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH
        export GTDBTK_DATA_PATH={params.db}

        mkdir -p {params.outdir}
        gtdbtk classify_wf \
            --genome_dir {input.bins} \
            --out_dir {params.outdir} \
            --extension fa \
            --cpus {threads} \
            --skip_ani_screen \
            2>&1 | tee {log}

        touch {params.outdir}/gtdbtk.ar53.summary.tsv
        touch {params.outdir}/gtdbtk.bac120.summary.tsv
        """


rule summary:
    """Merge CheckM2, GUNC, GTDB-Tk, and damage into a single summary TSV"""
    input:
        checkm      = f"{OUTDIR}/checkm2_final/quality_report.tsv",
        gunc        = f"{OUTDIR}/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv",
        gtdbtk_ar53  = f"{OUTDIR}/gtdbtk/gtdbtk.ar53.summary.tsv",
        gtdbtk_bac120 = f"{OUTDIR}/gtdbtk/gtdbtk.bac120.summary.tsv",
        damage      = f"{RESOLVE_DIR}/damage_per_bin.tsv"
    output:
        summary = f"{OUTDIR}/summary.tsv"
    run:
        import pandas as pd

        checkm = pd.read_csv(input.checkm, sep='\t')
        checkm = checkm[['Name', 'Completeness', 'Contamination', 'Genome_Size', 'GC_Content']]
        checkm.columns = ['bin', 'completeness', 'contamination', 'genome_size', 'gc_content']

        gunc = pd.read_csv(input.gunc, sep='\t')
        gunc = gunc[['genome', 'clade_separation_score', 'pass.GUNC']]
        gunc.columns = ['bin', 'gunc_css', 'gunc_pass']

        dfs = []
        for path in [input.gtdbtk_ar53, input.gtdbtk_bac120]:
            try:
                df = pd.read_csv(path, sep='\t')
                if len(df) > 0:
                    dfs.append(df[['user_genome', 'classification']])
            except Exception:
                pass
        if dfs:
            gtdbtk = pd.concat(dfs).rename(
                columns={'user_genome': 'bin', 'classification': 'gtdb_classification'})
        else:
            gtdbtk = pd.DataFrame(columns=['bin', 'gtdb_classification'])

        damage = pd.read_csv(input.damage, sep='\t')
        keep = [c for c in ['bin', 'p_ancient', 'ct_1p', 'ga_1p', 'damage_class']
                if c in damage.columns]
        damage = damage[keep]

        summary = checkm.merge(gunc, on='bin', how='left')
        summary = summary.merge(gtdbtk, on='bin', how='left')
        summary = summary.merge(damage, on='bin', how='left')

        summary['quality'] = 'LQ'
        summary.loc[
            (summary.completeness >= 50) & (summary.contamination < 10), 'quality'] = 'MQ'
        summary.loc[
            (summary.completeness >= 90) & (summary.contamination < 5), 'quality'] = 'HQ'

        summary = summary.sort_values(
            ['quality', 'completeness'], ascending=[False, False])
        summary.to_csv(output.summary, sep='\t', index=False)

        hq = (summary.quality == 'HQ').sum()
        mq = (summary.quality == 'MQ').sum()
        anc = summary.get('damage_class', pd.Series(dtype=str)).eq('ancient').sum()
        print(f"\nSummary: {hq} HQ, {mq} MQ bins; {anc} ancient")


# ---------------------------------------------------------------------------
# PART C: Ancient Bin Selection + Deconvolution
# ---------------------------------------------------------------------------

checkpoint select_ancient_bin:
    """Select the Bog-38 HQ GUNC-pass bin for deconvolution"""
    input:
        checkm      = f"{OUTDIR}/checkm2_final/quality_report.tsv",
        gunc        = f"{OUTDIR}/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv",
        gtdbtk_ar53  = f"{OUTDIR}/gtdbtk/gtdbtk.ar53.summary.tsv",
        gtdbtk_bac120 = f"{OUTDIR}/gtdbtk/gtdbtk.bac120.summary.tsv"
    output:
        selected_bin = f"{OUTDIR}/selected_bin.txt"
    params:
        gtdbtk_clade     = config["ancient_selection"]["gtdbtk_clade"],
        min_completeness = config["ancient_selection"]["min_completeness"],
        max_contamination = config["ancient_selection"]["max_contamination"],
        require_gunc_pass = config["ancient_selection"]["require_gunc_pass"],
        tiebreak         = config["ancient_selection"]["tiebreak"]
    script:
        "scripts/select_ancient_bin.py"


def get_ancient_bin_fa(wildcards):
    ck = checkpoints.select_ancient_bin.get()
    bin_name = open(ck.output.selected_bin).read().strip()
    return f"{RESOLVE_BINS}/{bin_name}.fa"


rule deconvolve:
    """Run amber deconvolve on the selected ancient bin"""
    input:
        bin_fa   = get_ancient_bin_fa,
        bam      = config["bam"],
        selected = f"{OUTDIR}/selected_bin.txt"
    output:
        ancient = f"{OUTDIR}/deconvolve/ancient_consensus.fa",
        modern  = f"{OUTDIR}/deconvolve/modern_consensus.fa",
        smiley  = f"{OUTDIR}/deconvolve/smiley_data.tsv",
        trace   = f"{OUTDIR}/deconvolve/deconvolve_trace.log",
        mask    = f"{OUTDIR}/deconvolve/high_confidence_mask.bed"
    params:
        amber        = config["amber"]["binary"],
        library_type = config["deconvolve"]["library_type"],
        em_iters     = config["deconvolve"]["em_iterations"],
        outdir       = f"{OUTDIR}/deconvolve",
        env          = ENV_ASSEMBLY
    threads: config["deconvolve"]["threads"]
    resources:
        mem_mb   = 32000,
        runtime  = 60,
        slurm_partition = "compregular"
    log: f"{OUTDIR}/logs/deconvolve.log"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

        {params.amber} deconvolve \
            --contigs {input.bin_fa} \
            --bam {input.bam} \
            --output {params.outdir} \
            --library-type {params.library_type} \
            --em-iterations {params.em_iters} \
            --write-position-stats \
            --threads {threads} \
            2>&1 | tee {log}
        """


rule fastani_deconvolved:
    """FastANI: compare deconvolved ancient/modern consensi against reference genomes.
    Shows improvement vs the raw mixed bin — ancient_consensus should have higher ANI
    to its closest relative than the pre-deconvolution mixture.
    """
    input:
        ancient = f"{OUTDIR}/deconvolve/ancient_consensus.fa",
        modern  = f"{OUTDIR}/deconvolve/modern_consensus.fa"
    output:
        ani = f"{OUTDIR}/fastani/deconvolved_vs_refs.tsv"
    params:
        ref_dir = config["reference_genomes_dir"]
    conda: "/maps/projects/fernandezguerra/apps/opt/conda/envs/assembly"
    threads: config["threads"]
    resources:
        mem_mb   = 8000,
        runtime  = 30,
        slurm_partition = "compregular"
    log: f"{OUTDIR}/logs/fastani_deconvolved.log"
    shell:
        """
        mkdir -p $(dirname {output.ani})
        printf "%s\n%s\n" {input.ancient} {input.modern} > {output.ani}.ql
        find {params.ref_dir} -name "*.fna" -o -name "*.fa" > {output.ani}.rl
        fastANI --ql {output.ani}.ql --rl {output.ani}.rl \
            -o {output.ani} -t {threads} 2> {log}
        rm -f {output.ani}.ql {output.ani}.rl
        """


# ---------------------------------------------------------------------------
# PART D: Phylogenomics — genome setup
# ---------------------------------------------------------------------------

rule link_ref_genome:
    """Symlink a reference genome into the phylo genomes directory"""
    input:
        lambda wc: str(next(
            (p for ext in [".fna", ".fa"]
             for p in [_ref_dir / f"{wc.sample}{ext}"] if p.exists()),
            _ref_dir / f"{wc.sample}.fna"
        ))
    output:
        f"{PHYLO_DIR}/01_genomes/{{sample}}.fna"
    shell:
        "mkdir -p $(dirname {output}) && ln -sf $(realpath {input}) {output}"


rule link_ancient_consensus:
    input:  f"{OUTDIR}/deconvolve/ancient_consensus.fa"
    output: f"{PHYLO_DIR}/01_genomes/ancient_consensus.fa"
    shell:  "mkdir -p $(dirname {output}) && ln -sf $(realpath {input}) {output}"


rule link_modern_consensus:
    input:  f"{OUTDIR}/deconvolve/modern_consensus.fa"
    output: f"{PHYLO_DIR}/01_genomes/modern_consensus.fa"
    shell:  "mkdir -p $(dirname {output}) && ln -sf $(realpath {input}) {output}"


def get_phylo_genome_path(wildcards):
    if wildcards.sample in ["ancient_consensus", "modern_consensus", "s17_ancient_relative"]:
        return f"{PHYLO_DIR}/01_genomes/{wildcards.sample}.fa"
    return f"{PHYLO_DIR}/01_genomes/{wildcards.sample}.fna"


# ---------------------------------------------------------------------------
# PART E: Marker-gene phylogeny (Stage 1)
# ---------------------------------------------------------------------------

rule prodigal:
    """Predict protein-coding genes (metagenomic mode)"""
    input:
        genome = get_phylo_genome_path
    output:
        faa = f"{PHYLO_DIR}/02_prodigal/{{sample}}.faa",
        fna = f"{PHYLO_DIR}/02_prodigal/{{sample}}.fna",
        gff = f"{PHYLO_DIR}/02_prodigal/{{sample}}.gff"
    params:
        env = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/prodigal/{{sample}}.log"
    resources:
        mem_mb = 4000,
        runtime = 20,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        prodigal -i {input.genome} -a {output.faa} -d {output.fna} \
            -f gff -o {output.gff} -p meta -g 11 -q 2> {log}
        """


rule hmmsearch:
    """Search all marker genes against one sample in a single run"""
    input:
        faa    = f"{PHYLO_DIR}/02_prodigal/{{sample}}.faa",
        hmm_db = f"{config['marker_hmm_dir']}/all_markers.hmm"
    output:
        tbl = f"{PHYLO_DIR}/03_hmm/{{sample}}.tbl",
        out = f"{PHYLO_DIR}/03_hmm/{{sample}}.out"
    params:
        evalue = config["hmm_evalue"],
        env    = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/hmmsearch/{{sample}}.log"
    resources:
        mem_mb = 4000,
        runtime = 20,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        hmmsearch --tblout {output.tbl} -o {output.out} \
            -E {params.evalue} {input.hmm_db} {input.faa} 2> {log}
        """


rule extract_markers:
    """Extract best-hit marker sequences per sample"""
    input:
        faa = f"{PHYLO_DIR}/02_prodigal/{{sample}}.faa",
        tbl = f"{PHYLO_DIR}/03_hmm/{{sample}}.tbl"
    output:
        f"{PHYLO_DIR}/04_extracted/{{sample}}_markers.faa"
    params:
        markers = MARKERS,
        env     = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk",
        script  = workflow.basedir + "/scripts/extract_markers.py"
    log: f"{PHYLO_DIR}/logs/extract/{{sample}}.log"
    resources:
        mem_mb = 2000,
        runtime = 5,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        python {params.script} \
            --faa {input.faa} \
            --tbl {input.tbl} \
            --output {output} \
            --sample {wildcards.sample} \
            --log {log} \
            --markers {params.markers}
        """


rule align_marker:
    """Align each marker across all samples with MAFFT"""
    input:
        seqs = expand(
            f"{PHYLO_DIR}/04_extracted/{{sample}}_markers.faa",
            sample=PHYLO_SAMPLES
        )
    output:
        f"{PHYLO_DIR}/05_aligned/{{marker}}.aln.faa"
    params:
        marker    = "{marker}",
        algorithm = config["mafft_algorithm"],
        tmpfile   = f"{PHYLO_DIR}/05_aligned/{{marker}}.tmp.faa",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/phylogenomics_gb"
    log: f"{PHYLO_DIR}/logs/align/{{marker}}.log"
    threads: 4
    resources:
        mem_mb = 8000,
        runtime = 60,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}

        > {params.tmpfile}
        for f in {input.seqs}; do
            grep -A1 "_{params.marker}\\( \\|$\\)" "$f" >> {params.tmpfile} || true
        done

        mafft --maxiterate 1000 --localpair --thread {threads} \
            {params.tmpfile} > {output} 2> {log}

        rm -f {params.tmpfile}
        """


rule trim_alignment:
    """Trim alignments with BMGE"""
    input:
        f"{PHYLO_DIR}/05_aligned/{{marker}}.aln.faa"
    output:
        f"{PHYLO_DIR}/06_trimmed/{{marker}}.trimmed.faa"
    params:
        matrix = config["bmge_matrix"],
        env    = "/maps/projects/fernandezguerra/apps/opt/conda/envs/phylogenomics_gb"
    log: f"{PHYLO_DIR}/logs/trim/{{marker}}.log"
    resources:
        mem_mb = 4000,
        runtime = 10,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        if [ -s {input} ]; then
            bmge -i {input} -t AA -m {params.matrix} -of {output} 2> {log} || touch {output}
        else
            touch {output}
            echo "Empty input alignment" > {log}
        fi
        """


rule back_translate:
    """Back-translate AA alignment to codon alignment"""
    input:
        aa_aln  = f"{PHYLO_DIR}/06_trimmed/{{marker}}.trimmed.faa",
        nt_seqs = expand(
            f"{PHYLO_DIR}/02_prodigal/{{sample}}.fna",
            sample=PHYLO_SAMPLES
        )
    output:
        f"{PHYLO_DIR}/06_trimmed/{{marker}}.codon.fna"
    params:
        env    = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk",
        script = workflow.basedir + "/scripts/back_translate.py"
    log: f"{PHYLO_DIR}/logs/backtranslate/{{marker}}.log"
    resources:
        mem_mb = 4000,
        runtime = 10,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        python {params.script} \
            --aa-aln {input.aa_aln} \
            --output {output} \
            --log {log} \
            --nt-seqs {input.nt_seqs}
        """


rule concatenate_aa:
    """Concatenate untrimmed AA alignments into protein supermatrix.
    BMGE is applied AFTER concatenation (trim_concat_aa) to preserve
    signal — per-marker BMGE on short alignments is too aggressive and
    discards ~74% of sites vs concatenated BMGE (2204 vs 8549 cols).
    Per-marker trim_alignment is kept only for the codon/back-translate path.
    """
    input:
        expand(f"{PHYLO_DIR}/05_aligned/{{marker}}.aln.faa", marker=MARKERS)
    output:
        f"{PHYLO_DIR}/07_concatenated/concat_aa.faa"
    params:
        samples = PHYLO_SAMPLES
    log: f"{PHYLO_DIR}/logs/concatenate_aa.log"
    resources:
        mem_mb = 8000,
        runtime = 10,
        slurm_partition = "compregular"
    script:
        "scripts/concatenate_aa.py"


rule trim_concat_aa:
    """Apply BMGE to the concatenated AA supermatrix.
    Matches Guillaume Borrel's approach: trim after concatenation so the
    entropy calculation is stable across the full alignment (~8000 sites)
    rather than per-marker (~60 sites after aggressive trimming).
    """
    input:
        f"{PHYLO_DIR}/07_concatenated/concat_aa.faa"
    output:
        f"{PHYLO_DIR}/07_concatenated/concat_aa.bmge.faa"
    params:
        matrix = config["bmge_matrix"],
        env    = "/maps/projects/fernandezguerra/apps/opt/conda/envs/phylogenomics_gb"
    log: f"{PHYLO_DIR}/logs/trim_concat_aa.log"
    resources:
        mem_mb = 8000,
        runtime = 10,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        bmge -i {input} -t AA -m {params.matrix} -of {output} 2> {log}
        """


rule iqtree_aa:
    """Build ML phylogeny with IQ-TREE on protein supermatrix (LG+F+R10)"""
    input:
        f"{PHYLO_DIR}/07_concatenated/concat_aa.bmge.faa"
    output:
        tree = f"{PHYLO_DIR}/08_trees/aa_ml_tree.treefile",
        log_ = f"{PHYLO_DIR}/08_trees/aa_ml_tree.log"
    params:
        bootstrap = config["iqtree_bootstrap"],
        alrt      = config["iqtree_alrt"],
        prefix    = f"{PHYLO_DIR}/08_trees/aa_ml_tree",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_aa.log"
    threads: config["threads"]
    resources:
        mem_mb = 32000,
        runtime = 480,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input} \
            -m LG+F+R10 \
            -bb {params.bootstrap} -alrt {params.alrt} \
            -nt {threads} \
            --prefix {params.prefix} --redo \
            2> {log}
        echo "Sites used: $(awk 'NR==2{{print length($0)}}' {input}) AA columns" >> {log}
        """


rule concatenate_codon:
    """Concatenate all marker codon alignments into a single supermatrix"""
    input:
        expand(f"{PHYLO_DIR}/06_trimmed/{{marker}}.codon.fna", marker=MARKERS)
    output:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.txt"
    params:
        samples = PHYLO_SAMPLES
    log: f"{PHYLO_DIR}/logs/concatenate.log"
    resources:
        mem_mb = 8000,
        runtime = 10,
        slurm_partition = "compregular"
    script:
        "scripts/concatenate_codon.py"


rule trim_concat_codon:
    """Codon-aware gap filter on concatenated codon supermatrix.
    Removes codon triplets (3-nt units) where gap fraction exceeds max_gap,
    preserving reading frame. Outputs updated partition file with corrected
    marker boundaries. Mirrors the post-concatenation BMGE approach used for
    the AA tree to retain maximum signal.
    """
    input:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.txt"
    output:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.trimmed.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.trimmed.txt"
    params:
        max_gap = config.get("codon_max_gap", 0.2)
    log: f"{PHYLO_DIR}/logs/trim_concat_codon.log"
    resources:
        mem_mb = 4000,
        runtime = 5,
        slurm_partition = "compregular"
    script:
        "scripts/codon_bmge.py"


rule codon_12only:
    """Strip 3rd codon positions from concatenated codon supermatrix.
    3rd positions are saturated at Methanoflorentales divergence distances
    (entropy=1.46 vs 1.0 for 1st+2nd), adding noise rather than signal.
    Dropping them recovers AA-level support values (SH-aLRT ~74% vs ~56%).
    """
    input:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.txt"
    output:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.12only.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.12only.txt"
    params:
        max_gap     = config.get("codon_max_gap", 0.2),
        strip_third = True
    log: f"{PHYLO_DIR}/logs/codon_12only.log"
    resources:
        mem_mb = 4000,
        runtime = 5,
        slurm_partition = "compregular"
    script:
        "scripts/codon_bmge.py"


rule iqtree_codon_12:
    """ML tree on 1st+2nd codon positions only (3rd positions removed).
    Serves as a robustness check: if SOIL placement support recovers to ~74%,
    it confirms 3rd position saturation was the cause of weak support.
    """
    input:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.12only.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.12only.txt"
    output:
        tree = f"{PHYLO_DIR}/08_trees/codon12_ml_tree.treefile",
        log_ = f"{PHYLO_DIR}/08_trees/codon12_ml_tree.log"
    params:
        model     = config["iqtree_model"],
        bootstrap = config["iqtree_bootstrap"],
        alrt      = config["iqtree_alrt"],
        prefix    = f"{PHYLO_DIR}/08_trees/codon12_ml_tree",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_codon12.log"
    threads: config["threads"]
    resources:
        mem_mb = 32000,
        runtime = 240,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input.aln} -p {input.partitions} \
            -m {params.model} \
            -bb {params.bootstrap} -alrt {params.alrt} \
            -nt {threads} \
            --prefix {params.prefix} --redo \
            2> {log}
        """


rule iqtree_codon:
    """Build ML phylogeny with IQ-TREE on codon supermatrix"""
    input:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.trimmed.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.trimmed.txt"
    output:
        tree = f"{PHYLO_DIR}/08_trees/codon_ml_tree.treefile",
        log_ = f"{PHYLO_DIR}/08_trees/codon_ml_tree.log"
    params:
        model     = config["iqtree_model"],
        bootstrap = config["iqtree_bootstrap"],
        alrt      = config["iqtree_alrt"],
        prefix    = f"{PHYLO_DIR}/08_trees/codon_ml_tree",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_codon.log"
    threads: config["threads"]
    resources:
        mem_mb = 32000,
        runtime = 240,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input.aln} -p {input.partitions} \
            -m {params.model} \
            -bb {params.bootstrap} -alrt {params.alrt} \
            -nt {threads} \
            --prefix {params.prefix} --redo \
            2> {log}
        """


rule iqtree_codon_scf:
    """Compute site concordance factors (sCF) on the codon ML tree.
    sCF counts what fraction of parsimony-informative sites support each
    branch vs the two alternatives — more robust than UFBoot for short
    internodes in concatenated alignments.
    Output tree annotates nodes as SH-aLRT/UFBoot|sCF.
    """
    input:
        aln        = f"{PHYLO_DIR}/07_concatenated/concat_codon.trimmed.fna",
        partitions = f"{PHYLO_DIR}/07_concatenated/partitions.trimmed.txt",
        tree       = f"{PHYLO_DIR}/08_trees/codon_ml_tree.treefile"
    output:
        cf_tree = f"{PHYLO_DIR}/08_trees/codon_scf.cf.tree",
        cf_stat = f"{PHYLO_DIR}/08_trees/codon_scf.cf.stat"
    params:
        n_quartet = config.get("iqtree_scf_quartets", 100),
        prefix    = f"{PHYLO_DIR}/08_trees/codon_scf",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_codon_scf.log"
    threads: config["threads"]
    resources:
        mem_mb   = 32000,
        runtime  = 120,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input.aln} -p {input.partitions} \
            -t {input.tree} \
            --scf {params.n_quartet} \
            -nt {threads} \
            --prefix {params.prefix} --redo \
            2> {log}
        """


rule beast2_xml:
    """Generate BEAST2 XML for tip-dating"""
    input:
        aln  = f"{PHYLO_DIR}/07_concatenated/concat_codon.trimmed.fna",
        tree = f"{PHYLO_DIR}/08_trees/codon_ml_tree.treefile"
    output:
        xml    = f"{PHYLO_DIR}/09_beast2/codon_tipdating.xml",
        script = f"{PHYLO_DIR}/09_beast2/run_beast.sh"
    params:
        chain_length     = config["beast2_chain_length"],
        log_every        = config["beast2_log_every"],
        tip_dates        = {s: (config["sample_age"] if s == "ancient_consensus" else 0)
                            for s in PHYLO_SAMPLES},
        clock_rate_mean  = config["clock_rate_prior_mean"],
        beagle_lib       = config["beagle_lib"]
    log: f"{PHYLO_DIR}/logs/beast2_xml.log"
    resources:
        mem_mb = 4000,
        runtime = 10,
        slurm_partition = "compregular"
    script:
        "scripts/create_beast2_xml.py"


rule beast2_run:
    """Run BEAST2 tip-dating analysis (2x A100 GPU via sbatch --wait)"""
    input:
        xml = f"{PHYLO_DIR}/09_beast2/codon_tipdating.xml"
    output:
        log_   = f"{PHYLO_DIR}/09_beast2/codon_tipdating.log",
        trees  = f"{PHYLO_DIR}/09_beast2/codon_tipdating.trees"
    params:
        beagle_lib = config["beagle_lib"],
        workdir    = f"{PHYLO_DIR}/09_beast2",
        beast2_bin = "/maps/projects/caeg/people/kbd606/scratch/kapk-assm/amber/beast2_2.7/beast/bin/beast",
        java_home  = "/maps/projects/fernandezguerra/apps/opt/conda/envs/java17/lib/jvm"
    log: f"{PHYLO_DIR}/logs/beast2_run.log"
    shell:
        """
        sbatch --wait \
            --job-name=beast2_2gpu \
            --partition=compregular \
            --gres=gpu:a100:2 \
            --cpus-per-task=32 \
            --mem=64G \
            --time=12:00:00 \
            --output={log} \
            --error={log} \
            --wrap='
                export JAVA_HOME="{params.java_home}"
                export PATH="{params.java_home}/bin:${{PATH}}"
                export LD_LIBRARY_PATH="{params.beagle_lib}:${{LD_LIBRARY_PATH:-}}"
                export LD_PRELOAD="{params.beagle_lib}/libhmsbeagle.so.1"
                cd {params.workdir}
                "{params.beast2_bin}" \
                    -beagle_GPU -beagle_order 1,2 -beagle_double \
                    -instances 2 -threads 32 \
                    -overwrite codon_tipdating.xml
            '
        """


rule treeannotator:
    """Summarise BEAST2 posterior trees into MCC tree (10% burnin)"""
    input:
        trees = f"{PHYLO_DIR}/09_beast2/codon_tipdating.trees"
    output:
        mcc = f"{PHYLO_DIR}/09_beast2/codon_tipdating_mcc.tree"
    params:
        burnin         = config.get("beast2_burnin_pct", 10),
        treeannotator  = "/maps/projects/caeg/people/kbd606/scratch/kapk-assm/amber/beast2_2.7/beast/bin/treeannotator",
        java_home      = "/maps/projects/fernandezguerra/apps/opt/conda/envs/java17/lib/jvm"
    log: f"{PHYLO_DIR}/logs/treeannotator.log"
    shell:
        """
        export JAVA_HOME="{params.java_home}"
        export PATH="{params.java_home}/bin:${{PATH}}"
        "{params.treeannotator}" \
            -burnin {params.burnin} \
            -height mean \
            {input.trees} {output.mcc} \
            > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# PART E2: SNP-based BEAST2 tip-dating (with ASC correction)
# ---------------------------------------------------------------------------

rule beast2_snp_xml:
    """Generate BEAST2 XML for SNP-based tip-dating with ASC correction"""
    input:
        aln  = f"{PHYLO_DIR}/10_snp_tree/snp_alignment_tvonly.fna",
        ref  = f"{PHYLO_DIR}/01_genomes/{config['snp_reference']}.fna",
        tree = f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_tree.contree"
    output:
        xml = f"{PHYLO_DIR}/09_beast2/snp_tipdating.xml"
    params:
        chain_length    = config["beast2_chain_length"],
        log_every       = config["beast2_log_every"],
        tip_dates       = {"ancient_consensus": config["sample_age"]},
        clock_rate_mean = config.get("beast2_snp_clock_rate_mean", 1e-9)
    log: f"{PHYLO_DIR}/logs/beast2_snp_xml.log"
    script:
        "scripts/create_beast2_snp_xml.py"


rule beast2_snp_run:
    """Run SNP-based BEAST2 tip-dating (2x A100 GPU via sbatch --wait)"""
    input:
        xml = f"{PHYLO_DIR}/09_beast2/snp_tipdating.xml"
    output:
        log_   = f"{PHYLO_DIR}/09_beast2/snp_tipdating.log",
        trees  = f"{PHYLO_DIR}/09_beast2/snp_tipdating.trees"
    params:
        beagle_lib = config["beagle_lib"],
        workdir    = f"{PHYLO_DIR}/09_beast2",
        beast2_bin = "/maps/projects/caeg/people/kbd606/scratch/kapk-assm/amber/beast2_2.7/beast/bin/beast",
        java_home  = "/maps/projects/fernandezguerra/apps/opt/conda/envs/java17/lib/jvm"
    log: f"{PHYLO_DIR}/logs/beast2_snp_run.log"
    shell:
        """
        sbatch --wait \
            --job-name=beast2_snp \
            --partition=compregular \
            --gres=gpu:a100:2 \
            --cpus-per-task=32 \
            --mem=64G \
            --time=12:00:00 \
            --output={log} \
            --error={log} \
            --wrap='
                export JAVA_HOME="{params.java_home}"
                export PATH="{params.java_home}/bin:${{PATH}}"
                export LD_LIBRARY_PATH="{params.beagle_lib}:${{LD_LIBRARY_PATH:-}}"
                export LD_PRELOAD="{params.beagle_lib}/libhmsbeagle.so.1"
                cd {params.workdir}
                "{params.beast2_bin}" \
                    -beagle_GPU -beagle_order 1,2 -beagle_double \
                    -instances 2 -threads 32 \
                    -overwrite snp_tipdating.xml
            '
        """


rule treeannotator_snp:
    """Summarise SNP BEAST2 posterior trees into MCC tree (10% burnin)"""
    input:
        trees = f"{PHYLO_DIR}/09_beast2/snp_tipdating.trees"
    output:
        mcc = f"{PHYLO_DIR}/09_beast2/snp_tipdating_mcc.tree"
    params:
        burnin        = config.get("beast2_burnin_pct", 10),
        treeannotator = "/maps/projects/caeg/people/kbd606/scratch/kapk-assm/amber/beast2_2.7/beast/bin/treeannotator",
        java_home     = "/maps/projects/fernandezguerra/apps/opt/conda/envs/java17/lib/jvm"
    log: f"{PHYLO_DIR}/logs/treeannotator_snp.log"
    shell:
        """
        export JAVA_HOME="{params.java_home}"
        export PATH="{params.java_home}/bin:${{PATH}}"
        "{params.treeannotator}" \
            -burnin {params.burnin} \
            -height mean \
            {input.trees} {output.mcc} \
            > {log} 2>&1
        """


# ---------------------------------------------------------------------------
# PART G: Extreme stasis validation
# ---------------------------------------------------------------------------

rule validate_stasis:
    """
    Extreme validation of evolutionary stasis claim:
    - SNP damage artifact check (C>T / G>A enrichment)
    - Pairwise distances ancient vs modern vs all taxa (codon + SNP)
    - Deconvolution quality metrics (damage rates, fragment lengths)
    """
    input:
        codon_aln   = f"{PHYLO_DIR}/07_concatenated/concat_codon.fna",
        snp_aln     = f"{PHYLO_DIR}/10_snp_tree/snp_alignment_tvonly.fna",
        snps_dir    = f"{PHYLO_DIR}/10_snp_tree/snps",
        deconv_dir  = f"{OUTDIR}/deconvolve",
        resolve_dir = RESOLVE_DIR
    output:
        report = f"{PHYLO_DIR}/validation/stasis_validation_report.txt"
    log: f"{PHYLO_DIR}/logs/validate_stasis.log"
    script:
        "scripts/validate_stasis.py"


# ---------------------------------------------------------------------------
# PART F: Core-genome SNP phylogeny (Stage 2)
# ---------------------------------------------------------------------------

rule nucmer_align:
    """Pairwise whole-genome alignment with nucmer"""
    input:
        ref   = f"{PHYLO_DIR}/01_genomes/{config['snp_reference']}.fna",
        query = get_phylo_genome_path
    output:
        delta    = f"{PHYLO_DIR}/10_snp_tree/nucmer/{{sample}}.delta",
        filtered = f"{PHYLO_DIR}/10_snp_tree/nucmer/{{sample}}.filter"
    params:
        prefix = f"{PHYLO_DIR}/10_snp_tree/nucmer/{{sample}}",
        minlen = config["nucmer_minmatch"],
        env    = "/maps/projects/fernandezguerra/apps/opt/conda/envs/bioinfo"
    log: f"{PHYLO_DIR}/logs/nucmer/{{sample}}.log"
    resources:
        mem_mb = 8000,
        runtime = 30,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        nucmer --maxmatch -p {params.prefix} {input.ref} {input.query} 2> {log}
        delta-filter -r -q -l {params.minlen} {output.delta} > {output.filtered} 2>> {log}
        """


rule extract_snps:
    """Extract SNPs and alignment coverage from nucmer alignments"""
    input:
        filtered = f"{PHYLO_DIR}/10_snp_tree/nucmer/{{sample}}.filter"
    output:
        snps   = f"{PHYLO_DIR}/10_snp_tree/snps/{{sample}}.snps",
        coords = f"{PHYLO_DIR}/10_snp_tree/snps/{{sample}}.coords"
    params:
        env = "/maps/projects/fernandezguerra/apps/opt/conda/envs/bioinfo"
    log: f"{PHYLO_DIR}/logs/snps/{{sample}}.log"
    resources:
        mem_mb = 4000,
        runtime = 10,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        show-snps -CT {input.filtered} > {output.snps} 2> {log}
        show-coords -r -T {input.filtered} > {output.coords} 2>> {log}
        """


rule build_snp_matrix:
    """Build SNP alignment matrix from all pairwise comparisons"""
    input:
        snps = expand(
            f"{PHYLO_DIR}/10_snp_tree/snps/{{sample}}.snps",
            sample=SPECIES_TAXA
        ),
        coords = expand(
            f"{PHYLO_DIR}/10_snp_tree/snps/{{sample}}.coords",
            sample=SPECIES_TAXA
        ),
        ref  = f"{PHYLO_DIR}/01_genomes/{config['snp_reference']}.fna",
        mask = f"{OUTDIR}/deconvolve/high_confidence_mask.bed"
    output:
        all_snps = f"{PHYLO_DIR}/10_snp_tree/snp_alignment_all.fna",
        tv_only  = f"{PHYLO_DIR}/10_snp_tree/snp_alignment_tvonly.fna",
        stats    = f"{PHYLO_DIR}/10_snp_tree/snp_stats.txt"
    params:
        taxa     = SPECIES_TAXA,
        ref_name = config["snp_reference"]
    log: f"{PHYLO_DIR}/logs/snp_matrix.log"
    resources:
        mem_mb = 8000,
        runtime = 10,
        slurm_partition = "compregular"
    script:
        "scripts/build_snp_matrix.py"


rule iqtree_snp_tvonly:
    """Build ML tree from transversions only (damage-robust)"""
    input:
        f"{PHYLO_DIR}/10_snp_tree/snp_alignment_tvonly.fna"
    output:
        f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_tree.treefile"
    params:
        model     = config["snp_tree_model"],
        bootstrap = config["iqtree_bootstrap"],
        prefix    = f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_tree",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_snp_tvonly.log"
    threads: 8
    resources:
        mem_mb = 16000,
        runtime = 120,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input} -m {params.model} \
            -bb {params.bootstrap} -nt {threads} \
            --prefix {params.prefix} --redo 2> {log}
        """


rule iqtree_snp_scf:
    """Compute site concordance factors on the transversion-only SNP tree.
    Resolves the SOIL100000032 placement where UFBoot=96 but SH-aLRT=56.4
    — sCF directly reports what fraction of TV-only sites support the
    branch, independent of bootstrap inflation from short internodes.
    """
    input:
        aln  = f"{PHYLO_DIR}/10_snp_tree/snp_alignment_tvonly.fna",
        tree = f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_tree.treefile"
    output:
        cf_tree = f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_scf.cf.tree",
        cf_stat = f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_scf.cf.stat"
    params:
        n_quartet = config.get("iqtree_scf_quartets", 100),
        model     = config["snp_tree_model"],
        prefix    = f"{PHYLO_DIR}/10_snp_tree/snp_tvonly_scf",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_snp_scf.log"
    threads: 8
    resources:
        mem_mb   = 16000,
        runtime  = 60,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input.aln} -m {params.model} \
            -t {input.tree} \
            --scf {params.n_quartet} \
            -nt {threads} \
            --prefix {params.prefix} --redo \
            2> {log}
        """


rule iqtree_snp_all:
    """Build ML tree from all SNPs"""
    input:
        f"{PHYLO_DIR}/10_snp_tree/snp_alignment_all.fna"
    output:
        f"{PHYLO_DIR}/10_snp_tree/snp_all_tree.treefile"
    params:
        model     = config["snp_tree_model"],
        bootstrap = config["iqtree_bootstrap"],
        prefix    = f"{PHYLO_DIR}/10_snp_tree/snp_all_tree",
        env       = "/maps/projects/fernandezguerra/apps/opt/conda/envs/gtdbtk"
    log: f"{PHYLO_DIR}/logs/iqtree_snp_all.log"
    threads: 8
    resources:
        mem_mb = 16000,
        runtime = 120,
        slurm_partition = "compregular"
    shell:
        """
        set +u
        source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
        conda activate {params.env}
        iqtree3 -s {input} -m {params.model} \
            -bb {params.bootstrap} -nt {threads} \
            --prefix {params.prefix} --redo 2> {log}
        """


# ---------------------------------------------------------------------------
# PART H: Incubation comparison — dRep dereplication + S17 read mapping
# ---------------------------------------------------------------------------

_INCUB = config.get("incubation", {})
_DREP  = config.get("drep", {})

if _INCUB and _DREP:

    rule incubation_compare:
        """Dereplicate paper + S17 bins; map S17 longDNA reads against the result"""
        input:
            f"{OUTDIR}/drep/s17_vs_drep.sorted.bam",
            f"{OUTDIR}/drep/s17_mapping_stats.txt"


    rule combine_incubation_bins:
        """Symlink paper HQ/MQ bins, deconvolved consensi, and S17 HQ/MQ bins into
        a single dir for dRep.
        Prefixes: paper_ · ancient_ · modern_ · s17hq_ · s17mq_
        """
        input:
            paper_bins   = get_hq_mq_bins,
            ancient      = f"{OUTDIR}/deconvolve/ancient_consensus.fa",
            modern       = f"{OUTDIR}/deconvolve/modern_consensus.fa",
            selected_bin = f"{OUTDIR}/selected_bin.txt",
            incub_hq     = _INCUB["hq_bins_dir"],
            incub_mq     = _INCUB["mq_bins_dir"]
        output:
            combined = directory(f"{OUTDIR}/drep/input_bins")
        run:
            import glob
            os.makedirs(output.combined, exist_ok=True)
            # Exclude the raw mixed bin — replaced by its deconvolved consensi
            excluded = open(input.selected_bin).read().strip() + ".fa"
            # Directory sources with prefix
            prefix_map = {
                input.paper_bins: "paper",
                input.incub_hq:   "s17hq",
                input.incub_mq:   "s17mq",
            }
            for src_dir, pfx in prefix_map.items():
                for src in sorted(glob.glob(f"{src_dir}/*.fa")):
                    if os.path.basename(src) == excluded and pfx == "paper":
                        continue
                    dst = os.path.join(output.combined, f"{pfx}_{os.path.basename(src)}")
                    if not os.path.exists(dst):
                        os.symlink(os.path.abspath(src), dst)
            # Deconvolved consensi replace the mixed bin
            for src, name in [(input.ancient, "ancient_consensus.fa"),
                               (input.modern,  "modern_consensus.fa")]:
                dst = os.path.join(output.combined, name)
                if not os.path.exists(dst):
                    os.symlink(os.path.abspath(src), dst)


    rule drep_dereplicate:
        """dRep species-level dereplication (95% primary / 99% secondary ANI).
        Manually copies winners to dereplicated_genomes/ (dRep v3.6.2 skips this
        when --ignoreGenomeQuality is used).
        """
        input:
            bins = f"{OUTDIR}/drep/input_bins"
        output:
            derep = directory(f"{OUTDIR}/drep/dereplicated_genomes")
        params:
            outdir = f"{OUTDIR}/drep",
            ani    = _DREP["ani"],
            s_ani  = _DREP["secondary_ani"],
            cov    = _DREP["min_overlap"],
            env    = _DREP["env"]
        threads: config["threads"]
        resources:
            mem_mb  = 32000,
            runtime = 120,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/drep.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}

            dRep dereplicate {params.outdir} \
                -g {input.bins}/*.fa \
                -pa {params.ani} \
                -sa {params.s_ani} \
                -nc {params.cov} \
                --ignoreGenomeQuality \
                -p {threads} \
                2>&1 | tee {log}

            # dRep v3.6.2 bug: winners not copied when --ignoreGenomeQuality
            mkdir -p {output.derep}
            python3 -c "
import csv, os, shutil
widb = '{params.outdir}/data_tables/Widb.csv'
src  = '{input.bins}'
dst  = '{output.derep}'
with open(widb) as f:
    for row in csv.DictReader(f):
        shutil.copy2(os.path.join(src, row['genome']), os.path.join(dst, row['genome']))
print(f'Copied {{len(os.listdir(dst))}} representative genomes to {output.derep}')
"
            """


    rule map_incubation_s17:
        """Map S17 longDNA reads against dereplicated MAG set (bwa-mem2)"""
        input:
            derep = f"{OUTDIR}/drep/dereplicated_genomes",
            r1    = _INCUB["reads_r1"],
            r2    = _INCUB["reads_r2"]
        output:
            bam   = f"{OUTDIR}/drep/s17_vs_drep.sorted.bam",
            bai   = f"{OUTDIR}/drep/s17_vs_drep.sorted.bam.bai",
            stats = f"{OUTDIR}/drep/s17_mapping_stats.txt"
        params:
            ref = f"{OUTDIR}/drep/derep_cat.fa",
            env = "/maps/projects/fernandezguerra/apps/opt/conda/envs/modern-meta"
        threads: config["threads"]
        resources:
            mem_mb  = 64000,
            runtime = 240,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/map_incubation_s17.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            cat {input.derep}/*.fa > {params.ref}
            bwa-mem2 index {params.ref} 2>> {log}
            bwa-mem2 mem -t {threads} {params.ref} {input.r1} {input.r2} \
                2>> {log} \
                | samtools sort -@ {threads} -m 2G -o {output.bam} 2>> {log}
            samtools index {output.bam} 2>> {log}
            samtools flagstat {output.bam} > {output.stats} 2>> {log}
            """


    rule extract_s17_ancient_reads:
        """Extract S17 reads mapped to ancient_consensus; convert to paired FASTQs.
        Filters to properly-paired reads with >=98% identity (qlen-NM)/qlen>=0.98.
        98% chosen over 97%: 0% contamination, 70% completeness, GUNC passes (CSS=0.32).
        """
        input:
            bam    = f"{OUTDIR}/drep/s17_vs_drep.sorted.bam",
            bai    = f"{OUTDIR}/drep/s17_vs_drep.sorted.bam.bai",
            genome = f"{OUTDIR}/drep/dereplicated_genomes/ancient_consensus.fa"
        output:
            r1 = f"{OUTDIR}/s17_assembly/reads/ancient_reads_R1.fastq.gz",
            r2 = f"{OUTDIR}/s17_assembly/reads/ancient_reads_R2.fastq.gz"
        params:
            env = ENV_ASSEMBLY
        threads: 8
        resources:
            mem_mb  = 16000,
            runtime = 60,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/extract_s17_ancient_reads.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}

            mkdir -p $(dirname {output.r1})
            grep "^>" {input.genome} | sed 's/>//' \
                > $(dirname {output.r1})/contigs.txt

            samtools view -@ {threads} -f 0x3 -e '(qlen-[NM])/qlen>=0.98' -b {input.bam} \
                $(cat $(dirname {output.r1})/contigs.txt | tr '\\n' ' ') \
                2>> {log} \
            | samtools sort -n -@ {threads} -m 2G 2>> {log} \
            | samtools fastq -@ {threads} \
                -1 {output.r1} -2 {output.r2} \
                -0 /dev/null -s /dev/null \
                2>> {log}
            """


    rule assemble_s17_ancient_relative:
        """metaSPAdes assembly of S17 reads from the ancient genome lineage.
        Produces the modern relative genome (t0, longDNA extraction) for
        inclusion in the phylogenetic tree and BEAST tip-dating.
        """
        input:
            r1 = f"{OUTDIR}/s17_assembly/reads/ancient_reads_R1.fastq.gz",
            r2 = f"{OUTDIR}/s17_assembly/reads/ancient_reads_R2.fastq.gz"
        output:
            fasta = f"{OUTDIR}/s17_assembly/assembly/final.contigs.fa"
        params:
            outdir = f"{OUTDIR}/s17_assembly/assembly",
            env    = ENV_ASSEMBLY
        threads: config["threads"]
        resources:
            mem_mb  = 64000,
            runtime = 240,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/assemble_s17_ancient_relative.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}

            rm -rf {params.outdir}
            spades.py --meta \
                -1 {input.r1} -2 {input.r2} \
                -o {params.outdir} \
                -t {threads} \
                -m {resources.mem_mb}000 \
                2>&1 | tee {log}
            mv {params.outdir}/scaffolds.fasta {output.fasta}
            """


    rule map_s17_to_ancient:
        """Map S17 ancient-lineage reads to ancient_consensus reference.
        Produces a BAM in reference coordinate space for consensus calling.
        Reads are already >=97% identity filtered by extract_s17_ancient_reads.
        """
        input:
            r1  = f"{OUTDIR}/s17_assembly/reads/ancient_reads_R1.fastq.gz",
            r2  = f"{OUTDIR}/s17_assembly/reads/ancient_reads_R2.fastq.gz",
            ref = f"{OUTDIR}/deconvolve/ancient_consensus.fa"
        output:
            bam = f"{OUTDIR}/s17_assembly/consensus/s17_vs_ancient.sorted.bam",
            bai = f"{OUTDIR}/s17_assembly/consensus/s17_vs_ancient.sorted.bam.bai"
        params:
            env = ENV_ASSEMBLY
        threads: config["threads"]
        resources:
            mem_mb  = 16000,
            runtime = 60,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/map_s17_to_ancient.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}

            mkdir -p $(dirname {output.bam})
            minimap2 -a -x sr -t {threads} {input.ref} {input.r1} {input.r2} \
                2>> {log} \
            | samtools sort -@ {threads} -m 2G -o {output.bam} 2>> {log}
            samtools index {output.bam} 2>> {log}
            """


    rule consensus_s17_ancient_relative:
        """Call reference-guided consensus of S17 ancient lineage from mapping to ancient_consensus.
        Reference-guided consensus preserves marker gene positions → proper phylogenomic placement.
        Uncovered positions (7.6% at >=97% id threshold) become N.
        """
        input:
            bam = f"{OUTDIR}/s17_assembly/consensus/s17_vs_ancient.sorted.bam",
            bai = f"{OUTDIR}/s17_assembly/consensus/s17_vs_ancient.sorted.bam.bai"
        output:
            fasta = f"{OUTDIR}/s17_assembly/consensus/s17_ancient_relative.fa"
        params:
            env = ENV_ASSEMBLY
        threads: 4
        resources:
            mem_mb  = 8000,
            runtime = 30,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/consensus_s17_ancient_relative.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}

            samtools consensus -m simple --min-depth 1 --min-BQ 15 -l 0 \
                -o {output.fasta} {input.bam} 2>> {log}
            """


    rule checkm2_s17_assembly:
        """CheckM2 quality assessment of the S17 ancient-relative assembly"""
        input:
            fasta = f"{OUTDIR}/s17_assembly/assembly/final.contigs.fa"
        output:
            report = f"{OUTDIR}/s17_assembly/checkm2/quality_report.tsv"
        params:
            bindir = f"{OUTDIR}/s17_assembly/bins",
            outdir = f"{OUTDIR}/s17_assembly/checkm2",
            env    = ENV_ASSEMBLY
        threads: config["checkm2"]["threads"]
        resources:
            mem_mb  = 32000,
            runtime = 60,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/checkm2_s17_assembly.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            mkdir -p {params.bindir}
            cp {input.fasta} {params.bindir}/s17_ancient_relative.fa

            checkm2 predict \
                --input {params.bindir} \
                --output-directory {params.outdir} \
                --extension fa \
                --threads {threads} \
                --force \
                2>&1 | tee {log}
            """


    rule gunc_s17_assembly:
        """GUNC chimera detection on S17 ancient-relative assembly"""
        input:
            fasta = f"{OUTDIR}/s17_assembly/assembly/final.contigs.fa"
        output:
            report = f"{OUTDIR}/s17_assembly/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv"
        params:
            bindir = f"{OUTDIR}/s17_assembly/bins",
            outdir = f"{OUTDIR}/s17_assembly/gunc",
            db     = config["gunc"]["db"],
            env    = ENV_GUNC
        threads: config["gunc"]["threads"]
        resources:
            mem_mb  = 32000,
            runtime = 120,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/gunc_s17_assembly.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            mkdir -p {params.bindir} {params.outdir}
            cp {input.fasta} {params.bindir}/s17_ancient_relative.fa

            gunc run \
                --input_dir {params.bindir} \
                --out_dir {params.outdir} \
                --db_file {params.db} \
                --threads {threads} \
                2>&1 | tee {log}
            """


    rule gtdbtk_s17_assembly:
        """GTDB-Tk taxonomic classification of S17 ancient-relative assembly"""
        input:
            fasta = f"{OUTDIR}/s17_assembly/assembly/final.contigs.fa"
        output:
            ar53   = f"{OUTDIR}/s17_assembly/gtdbtk/gtdbtk.ar53.summary.tsv",
            bac120 = f"{OUTDIR}/s17_assembly/gtdbtk/gtdbtk.bac120.summary.tsv"
        params:
            bindir = f"{OUTDIR}/s17_assembly/bins",
            outdir = f"{OUTDIR}/s17_assembly/gtdbtk",
            db     = config["gtdbtk"]["db"],
            env    = ENV_GTDBTK
        threads: config["gtdbtk"]["threads"]
        resources:
            mem_mb  = 128000,
            runtime = 240,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/gtdbtk_s17_assembly.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH
            export GTDBTK_DATA_PATH={params.db}

            rm -rf {params.outdir}
            mkdir -p {params.bindir} {params.outdir}
            cp {input.fasta} {params.bindir}/s17_ancient_relative.fa

            gtdbtk classify_wf \
                --genome_dir {params.bindir} \
                --out_dir {params.outdir} \
                --extension fa \
                --cpus {threads} \
                --skip_ani_screen \
                2>&1 | tee {log}

            touch {params.outdir}/gtdbtk.ar53.summary.tsv
            touch {params.outdir}/gtdbtk.bac120.summary.tsv
            """


    rule deconvolve_s17:
        """Run amber deconvolve on S17 reads mapped to ancient_consensus.
        Uses the original bwa-mem2 mapping against the dereplicated MAG set,
        subsetted to ancient_consensus contigs. Separates ancient (damaged) vs
        modern (undamaged) S17 signal; modern_consensus.fa is used for phylogenomics.
        """
        input:
            bam     = f"{OUTDIR}/drep/s17_vs_drep.sorted.bam",
            bai     = f"{OUTDIR}/drep/s17_vs_drep.sorted.bam.bai",
            contigs = f"{OUTDIR}/deconvolve/ancient_consensus.fa"
        output:
            ancient     = f"{OUTDIR}/s17_assembly/deconvolve/ancient_consensus.fa",
            modern      = f"{OUTDIR}/s17_assembly/deconvolve/modern_consensus.fa",
            smiley      = f"{OUTDIR}/s17_assembly/deconvolve/smiley_data.tsv",
            trace       = f"{OUTDIR}/s17_assembly/deconvolve/deconvolve_trace.log",
            mask        = f"{OUTDIR}/s17_assembly/deconvolve/high_confidence_mask.bed",
            modern_bam  = f"{OUTDIR}/s17_assembly/deconvolve/modern_reads.bam"
        params:
            amber        = config["amber"]["binary"],
            library_type = config["deconvolve"]["library_type"],
            em_iters     = config["deconvolve"]["em_iterations"],
            outdir       = f"{OUTDIR}/s17_assembly/deconvolve",
            env          = ENV_ASSEMBLY
        threads: config["deconvolve"]["threads"]
        resources:
            mem_mb          = 32000,
            runtime         = 60,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/deconvolve_s17.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            mkdir -p {params.outdir}

            # Subset drep BAM to ancient_consensus contigs, >=98% identity filter
            # (validated by CheckM2/GUNC: removes related-organism reads that
            # competitively mapped to ancient_consensus at 90-97% identity)
            SUBSET_BAM={params.outdir}/s17_vs_ancient_subset.sorted.bam
            CONTIGS=$(grep "^>" {input.contigs} | sed 's/>//' | tr '\\n' ' ')
            samtools view -@ {threads} -f 0x3 -e '(qlen-[NM])/qlen>=0.98' -b {input.bam} $CONTIGS \
                | samtools sort -@ {threads} -m 2G -o $SUBSET_BAM
            samtools index $SUBSET_BAM

            {params.amber} deconvolve \
                --contigs {input.contigs} \
                --bam $SUBSET_BAM \
                --output {params.outdir} \
                --library-type {params.library_type} \
                --em-iterations {params.em_iters} \
                --write-position-stats \
                --write-modern-bam \
                --threads {threads} \
                2>&1 | tee {log}
            """


    rule assemble_s17_modern:
        """De novo metaSPAdes assembly of EM-classified modern reads from S17.
        Avoids reference bias of the reference-guided modern_consensus.fa approach.
        Input: modern_reads.bam (p_ancient <= 0.2 from amber deconvolve EM).
        """
        input:
            bam = f"{OUTDIR}/s17_assembly/deconvolve/modern_reads.bam"
        output:
            fasta = f"{OUTDIR}/s17_assembly/deconvolve/modern_assembly/scaffolds.fasta"
        params:
            outdir = f"{OUTDIR}/s17_assembly/deconvolve/modern_assembly",
            env    = ENV_ASSEMBLY
        threads: config["threads"]
        resources:
            mem_mb          = 64000,
            runtime         = 240,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/assemble_s17_modern.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}

            samtools sort -n -@ {threads} -m 2G {input.bam} \
                | samtools fastq -@ {threads} \
                    -1 {params.outdir}_R1.fastq.gz \
                    -2 {params.outdir}_R2.fastq.gz \
                    -0 /dev/null -s /dev/null \
                    2>> {log}

            rm -rf {params.outdir}
            spades.py --meta \
                -1 {params.outdir}_R1.fastq.gz \
                -2 {params.outdir}_R2.fastq.gz \
                -o {params.outdir} \
                -t {threads} \
                -m {resources.mem_mb}000 \
                2>&1 | tee -a {log}
            """


    rule checkm2_s17_deconvolve:
        """CheckM2 quality assessment of the S17 deconvolved modern consensus."""
        input:
            fasta = f"{OUTDIR}/s17_assembly/deconvolve/modern_consensus.fa"
        output:
            report = f"{OUTDIR}/s17_assembly/deconvolve/checkm2/quality_report.tsv"
        params:
            bindir = f"{OUTDIR}/s17_assembly/deconvolve/bins",
            outdir = f"{OUTDIR}/s17_assembly/deconvolve/checkm2",
            env    = ENV_ASSEMBLY
        threads: config["checkm2"]["threads"]
        resources:
            mem_mb          = 32000,
            runtime         = 60,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/checkm2_s17_deconvolve.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            mkdir -p {params.bindir}
            cp {input.fasta} {params.bindir}/s17_modern.fa

            checkm2 predict \
                --input {params.bindir} \
                --output-directory {params.outdir} \
                --extension fa \
                --threads {threads} \
                --force \
                2>&1 | tee {log}
            """


    rule gunc_s17_deconvolve:
        """GUNC chimera detection on the S17 deconvolved modern consensus."""
        input:
            fasta = f"{OUTDIR}/s17_assembly/deconvolve/modern_consensus.fa"
        output:
            report = f"{OUTDIR}/s17_assembly/deconvolve/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv"
        params:
            bindir = f"{OUTDIR}/s17_assembly/deconvolve/bins",
            outdir = f"{OUTDIR}/s17_assembly/deconvolve/gunc",
            db     = config["gunc"]["db"],
            env    = ENV_GUNC
        threads: config["gunc"]["threads"]
        resources:
            mem_mb          = 32000,
            runtime         = 120,
            slurm_partition = "compregular"
        log: f"{OUTDIR}/logs/gunc_s17_deconvolve.log"
        shell:
            """
            set +u
            source /maps/projects/fernandezguerra/apps/opt/conda/etc/profile.d/conda.sh
            conda activate {params.env}
            export LD_LIBRARY_PATH={params.env}/lib:$LD_LIBRARY_PATH

            mkdir -p {params.bindir} {params.outdir}
            cp {input.fasta} {params.bindir}/s17_modern.fa

            gunc run \
                --input_dir {params.bindir} \
                --out_dir {params.outdir} \
                --db_file {params.db} \
                --threads {threads} \
                2>&1 | tee {log}
            """


    rule link_s17_assembly:
        """Symlink S17 deconvolved modern consensus into phylogenomics genome dir.
        Uses amber deconvolve modern output (undamaged long-fragment fraction)
        for proper ancient/modern separation — not samtools consensus.
        QC gates: CheckM2 >=90% completeness / <5% contamination, GUNC pass.
        """
        input:
            fasta   = f"{OUTDIR}/s17_assembly/deconvolve/modern_consensus.fa",
            checkm2 = f"{OUTDIR}/s17_assembly/deconvolve/checkm2/quality_report.tsv",
            gunc    = f"{OUTDIR}/s17_assembly/deconvolve/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv"
        output: f"{PHYLO_DIR}/01_genomes/s17_ancient_relative.fa"
        shell:  "mkdir -p $(dirname {output}) && ln -sf $(realpath {input.fasta}) {output}"


rule s17_assembly:
    """Extract S17 reads from ancient lineage, assemble, CheckM2, GUNC, GTDB-Tk, deconvolve"""
    input:
        f"{OUTDIR}/s17_assembly/checkm2/quality_report.tsv",
        f"{OUTDIR}/s17_assembly/gunc/GUNC.progenomes_2.1.maxCSS_level.tsv",
        f"{OUTDIR}/s17_assembly/gtdbtk/gtdbtk.ar53.summary.tsv",
        f"{OUTDIR}/s17_assembly/deconvolve/smiley_data.tsv" if config.get("incubation", {}).get("include_s17_assembly", False) else []
